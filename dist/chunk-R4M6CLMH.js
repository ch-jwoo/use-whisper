import { d } from './chunk-VO7VPLVP.js';
import { useEffectAsync, useMemoAsync } from '@chengsokdara/react-hooks-async';
import { useRef, useState, useEffect } from 'react';

var j={apiKey:"",autoStart:!1,timeSlice:1e3,onTranscribe:void 0,bufferSize:2},G={blob:void 0,text:void 0},Q=E=>{let{apiKey:d$1,autoStart:y,timeSlice:v,whisperConfig:s,onTranscribe:x,bufferSize:b}={...j,...E};if(!d$1&&!x)throw new Error("apiKey is required if onTranscribe is not provided");let c=useRef([]),a=useRef(),o=useRef(),r=useRef(),n=useRef(),[H,S]=useState(!1),m=useRef(!1),g=useRef(!1),[W,k]=useState(!1),[U,R]=useState(!1),[B,_]=useState(G);useEffect(()=>()=>{c.current&&(c.current=[]),a.current&&(a.current.flush(),a.current=void 0),r.current&&(r.current.destroy(),r.current=void 0),o.current&&(o.current.off("speaking",h),o.current.off("stopped_speaking",w)),n.current&&(n.current.getTracks().forEach(e=>e.stop()),n.current=void 0);},[]),useEffectAsync(async()=>{y&&await T();},[y]);let D=async()=>{await T();},F=async()=>{await q();},T=async()=>{try{if(n.current||await M(),n.current){if(!r.current){let{default:{RecordRTCPromisesHandler:t,StereoAudioRecorder:u}}=await import('recordrtc'),i={mimeType:"audio/wav",numberOfAudioChannels:1,recorderType:u,sampleRate:44100,timeSlice:v,type:"audio",ondataavailable:K};r.current=new t(n.current,i);}if(!a.current){let{Mp3Encoder:t}=await import('lamejs');a.current=new t(1,44100,96);}let e=await r.current.getState();(e==="inactive"||e==="stopped")&&await r.current.startRecording(),e==="paused"&&await r.current.resumeRecording(),k(!0);}}catch{}},M=async()=>{try{if(n.current&&n.current.getTracks().forEach(e=>e.stop()),n.current=await navigator.mediaDevices.getUserMedia({audio:!0}),!o.current){let{default:e}=await import('hark');o.current=e(n.current,{interval:100,play:!1}),o.current.on("speaking",h),o.current.on("stopped_speaking",w);}}catch{}},h=()=>{m.current=!0,S(!0);},w=()=>{m.current=!1,S(!1),g.current=!0;},q=async()=>{try{if(r.current){let e=await r.current.getState();(e==="recording"||e==="paused")&&await r.current.stopRecording(),z(),k(!1),await r.current.destroy(),c.current=[],a.current&&(a.current.flush(),a.current=void 0),r.current=void 0;}}catch{}},z=()=>{o.current&&(o.current.off("speaking",h),o.current.off("stopped_speaking",w),o.current=void 0),n.current&&(n.current.getTracks().forEach(e=>e.stop()),n.current=void 0);},K=async e=>{try{if(r.current){if(a.current){let u=await e.arrayBuffer(),i=a.current.encodeBuffer(new Int16Array(u)),p=new Blob([i],{type:"audio/mpeg"});m.current||b&&c.current.length>b&&c.current.shift(),c.current.push(p);}let t=await r.current.getState();if(g.current&&t==="recording"){let u=new Blob(c.current,{type:"audio/mpeg"}),i=new File([u],"speech.mp3",{type:"audio/mpeg"});g.current=!1,R(!0);let p=await O(i);p&&_(A=>({...A,text:p})),R(!1),c.current=[];}}}catch{}},O=useMemoAsync(async e=>{let t=new FormData;t.append("file",e),t.append("model","whisper-1");let u=s?.mode||"transcriptions";u==="transcriptions"&&t.append("language",s?.language??"en"),s?.prompt&&t.append("prompt",s.prompt),s?.response_format&&t.append("response_format",s.response_format),s?.temperature&&t.append("temperature",`${s.temperature}`);let i={};i["Content-Type"]="multipart/form-data",d$1&&(i.Authorization=`Bearer ${d$1}`);let{default:p}=await import('axios');return (await p.post(d+u,t,{headers:i})).data.text},[d$1,s]);return {recording:W,speaking:H,transcribing:U,transcript:B,startRecording:D,stopRecording:F}};

export { Q as a };
